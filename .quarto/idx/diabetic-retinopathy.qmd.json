{"title":"Diabetic Retinopathy Detection","markdown":{"yaml":{"title":"Diabetic Retinopathy Detection","author":"Deepak Jeyarajan","format":{"html":{"toc":true,"html-math-method":"katex","css":"styles.css"}},"date":"03/22/2024"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n::: {.callout-tip}\n## Beginner-Friendly\nNo prior knowledge is required for this article!\n:::\n- In the Overview, we break down the basics of deep learning and image recognition. We'll talk about the core concept, the math involved and how a basic model is made. Specifically, we'll focus on the model that Gargeya & Leng use in their highly-cited paper \"Automated Identification of Diabetic Retinopathy Using Deep Learning\". \n- The pertinent clinical takeaways are outlined in the Key Takeaways section.\n- In the Technical Overview, we dive under the hood to see how a basic model is trained with code! (coming soon).\n\n# Overview\nIn identifying diabetic retinopathy (DR), we traditionally interpret the retinal fundus, looking for characteristics like microaneurysms, neovascularisation and hard exudates. We'll explore how a model can accurately detect diabetic retinopathy without explicitly knowing that information.\n\n## The Basic Concept\nBroadly speaking, we're turning <b>data</b> (the fundal image) into <b>data</b> (a number saying if DR is present or not). The fundal image is itself made out of pixels, which are numbers. \n\nHere is information about 1 pixel in a zoomed in fundal picture. \n![Coordinates and RBG values](assets/rbg-channels2.png){.lightbox}\n\nThe values on the left (777 and 424) refer to the coordinates of the pixel. The values on the right refer to its \"Red, Blue, Green\" (RGB) values - how red, blue and green the pixel is out of a maximum of 255. Understandably, the fundus is more red than blue or green (181,95,48).\n\nThis is what it would look like if we split up the 3 colour channels. \n![alt text](assets/rbg-channels.png \"Red, Green & Blue Channels\")\n\n\nBut the main point is that an image is simply numbers!\nFor simplicity's sake, let's assume there is just 1 channel. \n\nThis is the essence of the whole model.\n![Our Goal Grossly Simplified](assets/DRorNot.png)\n\n\n## The Basic Math\nWe're going to use basic addition and multiplication to turn all these pixel numbers into a final number.\n\nThe first transformation we do is called a convolution (hence these models are called convolutional neural networks).\n\nYou start off with the image matrix and a kernal (a matrix of numbers)\n![Matrix & Kernal](assets/matrix-kernal.png)\n\nWe move a kernal along the image, multiplying each kernal's value by the pixel value and adding all of them up. \n![Convolution](assets/convolutiongif.gif)\n\n After we apply the kernal through the image, it gives us a new matrix that we call a feature map.\n![Feature Map](assets/matrix-kernal-to-features.png)\n\nNow, why is it called a feature map? A feature map maps out key features the model uses for classification. Let's run the number 7 from the famous MNIST database through the kernal, a large database of handwritten digits.\n![Number 7 using the Same Kernal](assets/mnist-vertical-lines.png)\n\nAs you can see, the our feature map has highlighted the vertical parts of the image. In a sense, the model has learned what vertical lines are! Let's use another kernal and see what happens.\n![Number 7 with a New Kernal](assets/mnist-horizontal-lines.png)\n\n::: {.callout-tip title=\"Tip\"}\nUsing the multiplication and addition steps we explained above, try to see why those two kernals generate their respective feature maps!\n:::\n\n## The Basic Model \nIn an actual model, the original image is transformed by numerous different kernals to form a variety of feature maps. The beauty of deep learning is that we don't have to set kernals for what features we think are important - the model does that by itself. Let's see how it does that. \n\n::: {.callout-note title=\"Jargon\"}\nThe secret behind machine learning (of which deep learning is a subset of) is \"gradient descent.\" We'll describe its intuition here.\n:::\n\nSuppose I have a deep learning model that tells me if a number is 7 or not. One catch is that all of numbers in the kernals are randomised, meaning no meaningful features are being identified - our model doesn't work. I draw the number 7 and test the model. It predicts 50-50 for if it is a 7 or not. But we know that the correct answer is 100-0 that it is a 7. Let's say we want to improve our model so we randomly tweak the kernals. Now the model's prediction is 60-40 in favour of 7. The model is improving!  \n\nSimply put, I want to figure out what number the kernal needs to be for our model's prediction to be the best. This is a calculus question. With basic calculus, you can derive the direction (increase or decrease) each kernal value needs to change to improve the prediction. Now, every time our model updates its kernal values, it is improving - it is learning!\n\nWe incrementally update the kernal values, recalculating the gradient at each step so we know which direction to move in. The rate at which we change the kernals is called the learning rate (more to come on this).\n![Gradient Descent](assets/gradient-descent.png)\n\nVoil√† - we are now training our model! In essence, we give the model the lot of practice questions (the data) and the answer sheet (the data's interpretation) and tell it to figure it out. This is both intelligent and not so intelligent. What if it sees a unique fundus that it has not been trained to see? Perhaps the innate features it has gathered is sufficient to remain accurate but perhaps it is not. Perhaps the dataset it is trained on is biased towards a particular demographic. Perhaps you work with a specific demographic of people and the original dataset it was trained on is too different.\n\nThis means that the training data must be heretogenous so it can work for a wide range of real-life cases. This makes it important to be critical of the sensitivity/specificity described by papers showcasing the model. Depending on the dataset the model was tested on, it may not be applicable to your patient. \n\nYou must be aware that the sensitivity/specificity described is based on a test dataset researchers have used (it varies from dataset to dataset) and ensure it is directly applicable to your patient. Here is a table from Gargeya & Leng:\n![Difference in Sensitivity & Specificity per Dataset](assets/difference-in-sens-spec.png). The model still performs extremely well but there is variable performance across datasets and data types ('mild vs severe' DR). \n\n# Key Takeaways\n::: {.callout-tip title=\"It is powerful\"}\nDeep Learning offers an extremely robust way to solve/answer any question by learning complex features invisible to us. There is an enormous variety of problems that can be solved by it (eg. risk of mortality, likelihood of falls, automation of administrative tasks, etc.)\n:::\n::: {.callout-warning title=\"Data heterogenicity is important\"}\nFor reliable and accurate performance, your input data must be familiar to the data the model was trained on. Therefore, data heterogenicity is important for the model's training. This also means you might need to interpret/fix your data to be in a way the model understands (eg. the fundal image needs to be taken with roughly the same way how the model is trained)\n:::\n::: {.callout-important title=\"Beware of overestimated sensitivity/specificities\"}\nBe aware of what datasets the model has been tested on - sufficient cross-validation is necessary to avoid bloated/overestimated specificity & sensitivity values. \n:::\n# Technical Overview\n    coming soon!","srcMarkdownNoYaml":"\n\n# Introduction\n::: {.callout-tip}\n## Beginner-Friendly\nNo prior knowledge is required for this article!\n:::\n- In the Overview, we break down the basics of deep learning and image recognition. We'll talk about the core concept, the math involved and how a basic model is made. Specifically, we'll focus on the model that Gargeya & Leng use in their highly-cited paper \"Automated Identification of Diabetic Retinopathy Using Deep Learning\". \n- The pertinent clinical takeaways are outlined in the Key Takeaways section.\n- In the Technical Overview, we dive under the hood to see how a basic model is trained with code! (coming soon).\n\n# Overview\nIn identifying diabetic retinopathy (DR), we traditionally interpret the retinal fundus, looking for characteristics like microaneurysms, neovascularisation and hard exudates. We'll explore how a model can accurately detect diabetic retinopathy without explicitly knowing that information.\n\n## The Basic Concept\nBroadly speaking, we're turning <b>data</b> (the fundal image) into <b>data</b> (a number saying if DR is present or not). The fundal image is itself made out of pixels, which are numbers. \n\nHere is information about 1 pixel in a zoomed in fundal picture. \n![Coordinates and RBG values](assets/rbg-channels2.png){.lightbox}\n\nThe values on the left (777 and 424) refer to the coordinates of the pixel. The values on the right refer to its \"Red, Blue, Green\" (RGB) values - how red, blue and green the pixel is out of a maximum of 255. Understandably, the fundus is more red than blue or green (181,95,48).\n\nThis is what it would look like if we split up the 3 colour channels. \n![alt text](assets/rbg-channels.png \"Red, Green & Blue Channels\")\n\n\nBut the main point is that an image is simply numbers!\nFor simplicity's sake, let's assume there is just 1 channel. \n\nThis is the essence of the whole model.\n![Our Goal Grossly Simplified](assets/DRorNot.png)\n\n\n## The Basic Math\nWe're going to use basic addition and multiplication to turn all these pixel numbers into a final number.\n\nThe first transformation we do is called a convolution (hence these models are called convolutional neural networks).\n\nYou start off with the image matrix and a kernal (a matrix of numbers)\n![Matrix & Kernal](assets/matrix-kernal.png)\n\nWe move a kernal along the image, multiplying each kernal's value by the pixel value and adding all of them up. \n![Convolution](assets/convolutiongif.gif)\n\n After we apply the kernal through the image, it gives us a new matrix that we call a feature map.\n![Feature Map](assets/matrix-kernal-to-features.png)\n\nNow, why is it called a feature map? A feature map maps out key features the model uses for classification. Let's run the number 7 from the famous MNIST database through the kernal, a large database of handwritten digits.\n![Number 7 using the Same Kernal](assets/mnist-vertical-lines.png)\n\nAs you can see, the our feature map has highlighted the vertical parts of the image. In a sense, the model has learned what vertical lines are! Let's use another kernal and see what happens.\n![Number 7 with a New Kernal](assets/mnist-horizontal-lines.png)\n\n::: {.callout-tip title=\"Tip\"}\nUsing the multiplication and addition steps we explained above, try to see why those two kernals generate their respective feature maps!\n:::\n\n## The Basic Model \nIn an actual model, the original image is transformed by numerous different kernals to form a variety of feature maps. The beauty of deep learning is that we don't have to set kernals for what features we think are important - the model does that by itself. Let's see how it does that. \n\n::: {.callout-note title=\"Jargon\"}\nThe secret behind machine learning (of which deep learning is a subset of) is \"gradient descent.\" We'll describe its intuition here.\n:::\n\nSuppose I have a deep learning model that tells me if a number is 7 or not. One catch is that all of numbers in the kernals are randomised, meaning no meaningful features are being identified - our model doesn't work. I draw the number 7 and test the model. It predicts 50-50 for if it is a 7 or not. But we know that the correct answer is 100-0 that it is a 7. Let's say we want to improve our model so we randomly tweak the kernals. Now the model's prediction is 60-40 in favour of 7. The model is improving!  \n\nSimply put, I want to figure out what number the kernal needs to be for our model's prediction to be the best. This is a calculus question. With basic calculus, you can derive the direction (increase or decrease) each kernal value needs to change to improve the prediction. Now, every time our model updates its kernal values, it is improving - it is learning!\n\nWe incrementally update the kernal values, recalculating the gradient at each step so we know which direction to move in. The rate at which we change the kernals is called the learning rate (more to come on this).\n![Gradient Descent](assets/gradient-descent.png)\n\nVoil√† - we are now training our model! In essence, we give the model the lot of practice questions (the data) and the answer sheet (the data's interpretation) and tell it to figure it out. This is both intelligent and not so intelligent. What if it sees a unique fundus that it has not been trained to see? Perhaps the innate features it has gathered is sufficient to remain accurate but perhaps it is not. Perhaps the dataset it is trained on is biased towards a particular demographic. Perhaps you work with a specific demographic of people and the original dataset it was trained on is too different.\n\nThis means that the training data must be heretogenous so it can work for a wide range of real-life cases. This makes it important to be critical of the sensitivity/specificity described by papers showcasing the model. Depending on the dataset the model was tested on, it may not be applicable to your patient. \n\nYou must be aware that the sensitivity/specificity described is based on a test dataset researchers have used (it varies from dataset to dataset) and ensure it is directly applicable to your patient. Here is a table from Gargeya & Leng:\n![Difference in Sensitivity & Specificity per Dataset](assets/difference-in-sens-spec.png). The model still performs extremely well but there is variable performance across datasets and data types ('mild vs severe' DR). \n\n# Key Takeaways\n::: {.callout-tip title=\"It is powerful\"}\nDeep Learning offers an extremely robust way to solve/answer any question by learning complex features invisible to us. There is an enormous variety of problems that can be solved by it (eg. risk of mortality, likelihood of falls, automation of administrative tasks, etc.)\n:::\n::: {.callout-warning title=\"Data heterogenicity is important\"}\nFor reliable and accurate performance, your input data must be familiar to the data the model was trained on. Therefore, data heterogenicity is important for the model's training. This also means you might need to interpret/fix your data to be in a way the model understands (eg. the fundal image needs to be taken with roughly the same way how the model is trained)\n:::\n::: {.callout-important title=\"Beware of overestimated sensitivity/specificities\"}\nBe aware of what datasets the model has been tested on - sufficient cross-validation is necessary to avoid bloated/overestimated specificity & sensitivity values. \n:::\n# Technical Overview\n    coming soon!"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"html-math-method":"katex","output-file":"diabetic-retinopathy.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","theme":"cosmo","toc-expand":true,"title":"Diabetic Retinopathy Detection","author":"Deepak Jeyarajan","date":"03/22/2024"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}