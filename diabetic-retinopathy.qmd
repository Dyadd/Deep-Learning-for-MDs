---
title: "Diabetic Retinopathy Detection"
author: "Deepak Jeyarajan"
format:
    html:
        toc: true
        html-math-method: katex
        css: styles.css
---

# Introduction
In this review, we break down the exact model that Gargeya & Leng use in their paper "Automated Identification of Diabetic Retinopathy Using Deep Learning". We explore a broad overview of what is happening here and then go into detail for a deeper insight. As always, it is not expected for you to know anything about Deep Learning and jargon is avoided/explained where appropriate. 

# Overview
- turning our data into numbers (pixels to numbers), let's keep in mind our end product: it is a probability of our fundus having DR or not - it is a number
- essentially we use basic math like addition and multiplication to turn all these numbers into a final number, which is our probability.
- multiplying and adding up our data by random numbers (x) â‡’ called a convolution 
    - gif of convolutions shrinking data
    - do you see how now our matrix shrinks all the way down to 1 number
- now we have our data, our convolutions and one number at the end
    - now we know what answer we want, now we can change our random kernel numbers to what will get us our answer!
    - it's sorta like looking at the answer sheet and changing our working out to get to the final answer
    - using basic calculus, we can figure out if each random number in each kernel needs to increase or decrease to get closer to the actual answer
        - the metric we call that we optimise for is the 'loss function'

# Fixing our Data

# Transforming our Data

# Ending with Probabilities

# Getting the Answer